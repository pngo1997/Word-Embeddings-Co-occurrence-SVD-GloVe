# ğŸ”  Word Embeddings: Co-occurrence, SVD & GloVe  

## ğŸ“œ Overview  
This project explores **word embeddings**, focusing on two key approaches:  

1. **Manually creating word embeddings** using a **co-occurrence matrix** and **Singular Value Decomposition (SVD)** for dimensionality reduction.  
2. **Experimenting with pre-trained GloVe embeddings**, verifying similarity relations, analogies, and inspecting ambiguous word embeddings.  

ğŸ“Œ **Key Concepts Covered**:  
- **Word Representations**: Understanding distributed word embeddings.  
- **Co-occurrence Matrix**: Capturing contextual relationships between words.  
- **Dimensionality Reduction**: Applying **Singular Value Decomposition (SVD)** to derive dense word vectors.  
- **Word Embedding Visualization**: Plotting embeddings in 2D space for interpretation.  
- **GloVe Embeddings**: Exploring **pre-trained embeddings**, analogy tasks, and ambiguity analysis.  

## ğŸš€ 1ï¸âƒ£ Implementation Details  

### **Part 1: Manually Creating Word Embeddings**  
âœ… **Build a Co-occurrence Matrix** from a given corpus.  
âœ… **Apply SVD** to reduce dimensionality and extract word vectors.  
âœ… **Visualize embeddings** in 2D space using **Matplotlib & PCA**.  
âœ… **Analyze Word Relationships**:  
   - Plot words with similar meanings in vector space.  
   - Observe clusters of related words.  

### **Part 2: Exploring GloVe Embeddings**  
âœ… **Load Pre-trained GloVe Embeddings**.  
âœ… **Reduce Dimensionality** using **PCA to 2D** for visualization.  
âœ… **Evaluate Word Similarities & Analogies**:  
   - Verify relationships between semantically similar words.  
   - Solve analogy tasks (`king - man + woman â‰ˆ queen`).
     
âœ… **Analyze Ambiguous Words**:  
   - Inspect word embeddings for words with multiple meanings (e.g., "bank" as a financial institution vs. riverbank).  

## ğŸ“Š 2ï¸âƒ£ Results & Visualizations  

### **Example: Word Embedding Plot**  
ğŸ“Œ **Visualization of word vectors in 2D space**:  
- Displays clusters of related words.  
- Shows how embeddings capture semantic relationships.  
- PCA is used to reduce high-dimensional embeddings.  

### **Example: Analogical Reasoning**  
ğŸ”¹ `king - man + woman â‰ˆ queen`  
ğŸ”¹ `Paris - France + Germany â‰ˆ Berlin`  
ğŸ”¹ `apple - fruit + vegetable â‰ˆ carrot`  

âœ… **The embeddings successfully capture analogical reasoning and semantic relationships**.  

## ğŸ“Œ Summary  
âœ… Implemented word embeddings from scratch using SVD.

âœ… Explored pre-trained GloVe embeddings.

âœ… Verified word relationships, analogies, and ambiguous words.

âœ… Visualized embeddings in 2D space for interpretation.
